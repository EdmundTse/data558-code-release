{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA558 Code Release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python implementation of a linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a linear SVM using fast gradient method\n",
    "We implement a linear support vector machine that uses a squared hinge loss function with an l2 regularisation term. We use the fast gradient method to minimise the loss function with respect to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svm(x, y, l, eta_init=1., max_iter=1000, eps=1e-2, include_steps=False):\n",
    "    \"\"\"Trains a two-class linear support vector machine using the squared hinge loss\n",
    "    function. This functions implements a fast gradient descent algorithm to optimise\n",
    "    the loss function with an l2 regularisation term.\n",
    "    \n",
    "    :param x: Training features, as a (d x n) 2d array\n",
    "    :param y: Training labels, as a (n x 1) 1d array of values +/- 1\n",
    "    :param l: Regularisation parameter; \"lambda\" in the equation\n",
    "    :param eta_init: The initial step size used in backtracking line search\n",
    "    :param max_iter: The maxmimum number of iterations for gradient descent\n",
    "    :param eps: The tolerance for the stopping criteron\n",
    "    :param include_steps: Whether to include the steps of intermediate steps of\n",
    "    iteratively optimising the decision vectors of the linear SVM.\n",
    "    \n",
    "    :returns: either the best trained weights for the input features, or if\n",
    "    include_steps is True, then also the sequence of trained weights from\n",
    "    every iteration of the fast gradient method.\n",
    "    \"\"\"\n",
    "    \n",
    "    if np.any(np.isin(np.unique(y), [-1, 1], invert=True)):\n",
    "        raise ValueError('labels of y must be either -1 or +1')\n",
    "    \n",
    "    d, n = x.shape\n",
    "    beta = np.zeros(d, dtype=x.dtype)\n",
    "    theta = np.zeros(d, dtype=x.dtype)\n",
    "    \n",
    "    grad = grad_squared_hinge(x, y, beta, l)\n",
    "    t = 0\n",
    "    betas = [beta]\n",
    "    while np.linalg.norm(grad) > eps and t < max_iter:\n",
    "        eta = backtracking(x, y, theta, l, eta_init)\n",
    "        new_beta = theta - eta * grad_squared_hinge(x, y, theta, l)\n",
    "        theta = new_beta + t / (t + 3) * (new_beta - beta)\n",
    "        beta = new_beta\n",
    "        betas.append(beta)\n",
    "        grad = grad_squared_hinge(x, y, beta, l)\n",
    "        t += 1\n",
    "    \n",
    "    if t == max_iter:\n",
    "        warnings.warn('SVM: Max iterations %d reached. l=%.2e, norm_grad=%.4f' % (max_iter, l, np.linalg.norm(grad)))\n",
    "    \n",
    "    return betas if include_steps else betas[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking line search\n",
    "For each iteration in our fast gradient method, we use backtracking line search to find the update step size. This algorithm is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(x, y, beta, l, eta, alpha=0.5, gamma=0.8, max_iter=100):\n",
    "    \"\"\"Perform backtracking line search\n",
    "    \n",
    "    :param x: Training features, as a (d x n) 2d array\n",
    "    :param y: Training labels, as a (n x 1) 1d array of values +/- 1\n",
    "    :param beta: The current point, as a vector in d-dimensions\n",
    "    :param l: Regularisation parameter; \"lambda\" in the equation\n",
    "    :param eta: The initial step size to use\n",
    "    :param alpha: A constant used to define when there is a sufficient decrease\n",
    "    :param gamma: Factor by which to decrease t at each backtracking iteration\n",
    "    :param max_iter: The maxmimum number of iterations of backtracking\n",
    "    :returns the step size to use\n",
    "    \"\"\"\n",
    "    obj_beta = squared_hinge(x, y, beta, l)\n",
    "    grad_beta = grad_squared_hinge(x, y, beta, l)\n",
    "    norm_grad_beta_squared = grad_beta.T @ grad_beta\n",
    "    found_eta = False\n",
    "    i = 0\n",
    "    while found_eta == False and i < max_iter:\n",
    "        lhs = squared_hinge(x, y, beta - eta * grad_beta, l)\n",
    "        rhs = obj_beta - alpha * eta * norm_grad_beta_squared\n",
    "        if lhs < rhs or np.isclose(lhs, rhs):\n",
    "            found_eta = True\n",
    "        else:\n",
    "            eta *= gamma\n",
    "            i += 1\n",
    "        \n",
    "    if i == max_iter:\n",
    "        raise Exception('BT: Max iterations reached.')\n",
    "    \n",
    "    if np.isclose(eta, 0):\n",
    "        warnings.warn('BT: Returned step size close to zero')\n",
    "    \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function: squared hinge loss with $\\ell_2$ regularisation\n",
    "Gradient descent requires us to compute the both the objective function and its gradient, which we implement below:\n",
    "\n",
    "$$\n",
    "F(\\beta) =\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\left( \\max(0, 1 - y_i x_i^T \\beta) \\right)^2 + \\lambda \\lVert \\beta \\rVert_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla F(\\beta)\n",
    "= - \\frac{2}{n} \\sum_{i=1}^n y_i x_i \\max(0, 1 - y_i x_i^T \\beta) + 2\\lambda\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_hinge(x, y, beta, l):\n",
    "    loss = 1 - y * (x.T @ beta)\n",
    "    loss = np.mean(np.power(loss * (loss > 0), 2))\n",
    "    penalty = l * (beta.T @ beta)\n",
    "    return loss + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using random seed: 7409\n",
      "Test using random seed: 7826\n",
      "Test using random seed: 4777\n"
     ]
    }
   ],
   "source": [
    "def test_squared_hinge(seed=None):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(10000)\n",
    "    \n",
    "    print('Test using random seed: %d' % seed)\n",
    "    np.random.seed(seed)\n",
    "    n = 100\n",
    "    d = 20\n",
    "\n",
    "    # When all beta is zero, the max would always give 1-0=1\n",
    "    x = np.random.normal(size=(d,n))\n",
    "    y = np.random.choice([1, -1], n).astype(x.dtype)\n",
    "    beta = np.zeros(d, dtype=x.dtype)\n",
    "    l = 0.\n",
    "    sum_yixi = sum([y[i] * x[:,i] for i in range(n)])\n",
    "    assert np.isclose(squared_hinge(x, y, beta, l),\n",
    "                      1 + l * np.linalg.norm(beta)**2)\n",
    "\n",
    "    # When y_i x_i^T beta >1, max should make it zero\n",
    "    x = np.random.uniform(1, 1234, size=(d,n))\n",
    "    y = np.ones(n, dtype=x.dtype)\n",
    "    beta = np.random.uniform(1, 10, d)\n",
    "    l = 0.\n",
    "    assert np.isclose(squared_hinge(x, y, beta, l), 0)\n",
    "\n",
    "    # When all x is zero, the max would always give 1-0=1\n",
    "    x = np.zeros((d,n), dtype=np.float64)\n",
    "    y = np.random.choice([1, -1], n).astype(x.dtype)\n",
    "    beta = np.random.normal(size=d)\n",
    "    l = np.random.randint(1234)\n",
    "    assert np.isclose(squared_hinge(x, y, beta, l),\n",
    "                      1 + l * np.linalg.norm(beta)**2)\n",
    "    \n",
    "    # Sum things the slow way\n",
    "    x = np.random.normal(size=(d,n))\n",
    "    y = np.random.choice([1, -1], n).astype(x.dtype)\n",
    "    beta = np.random.normal(size=d)\n",
    "    l = np.random.uniform(0, 10)\n",
    "    summation = sum([max(0, 1 - y[i] * x[:,i].dot(beta))**2 for i in range(n)])\n",
    "    soln = 1/n * summation + l * np.linalg.norm(beta)**2\n",
    "    assert np.isclose(squared_hinge(x, y, beta, l),\n",
    "                      soln)\n",
    "\n",
    "for i in range(3):\n",
    "    test_squared_hinge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_squared_hinge(x, y, beta, l):\n",
    "    yx = y * x\n",
    "    loss = 1 - yx.T @ beta\n",
    "    loss = -2 * np.mean(yx * loss * (loss > 0), axis=1)\n",
    "    penalty = 2 * l * beta\n",
    "    return loss + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using random seed: 957\n",
      "Test using random seed: 362\n",
      "Test using random seed: 4331\n"
     ]
    }
   ],
   "source": [
    "def test_grad_squared_hinge(seed=None):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(10000)\n",
    "        \n",
    "    print('Test using random seed: %d' % seed)\n",
    "    np.random.seed(seed)\n",
    "    n = 100\n",
    "    d = 20\n",
    "    \n",
    "    # When all x is zero, then we should only be up to the regularisation\n",
    "    x = np.zeros((d,n), dtype=np.float64)\n",
    "    y = np.random.choice([1, -1], n).astype(x.dtype)\n",
    "    beta = np.random.normal(size=d)\n",
    "    l = np.random.randint(1234)\n",
    "    np.testing.assert_allclose(grad_squared_hinge(x, y, beta, l), 2 * l * beta)\n",
    "    \n",
    "    # When all beta is zero, the max would always give 1-0=1\n",
    "    x = np.random.normal(size=(d,n))\n",
    "    y = np.random.choice([1, -1], n).astype(x.dtype)\n",
    "    beta = np.zeros(d)\n",
    "    l = 0\n",
    "    sum_yixi = sum([y[i] * x[:,i] for i in range(n)])\n",
    "    np.testing.assert_allclose(grad_squared_hinge(x, y, beta, l), -2 / n * sum_yixi)\n",
    "    \n",
    "    # When y_i x_i^T beta >1, max should make it zero\n",
    "    x = np.random.uniform(1, 1234, size=(d,n))\n",
    "    y = np.ones(n).astype(x.dtype)\n",
    "    beta = np.random.uniform(1, 10, d)\n",
    "    l = 0\n",
    "    np.testing.assert_allclose(grad_squared_hinge(x, y, beta, l), np.zeros(d))\n",
    "    \n",
    "    # Sum things the slow way\n",
    "    x = np.random.normal(size=(d,n))\n",
    "    y = np.random.choice([1, -1], n).astype(x.dtype)\n",
    "    beta = np.random.normal(size=d)\n",
    "    l = np.random.uniform(0, 10)\n",
    "    summation = sum([y[i] * x[:,i] * max(0, 1 - y[i] * x[:,i].dot(beta)) for i in range(n)])\n",
    "    soln = -2/n * summation + 2 * l * beta\n",
    "    np.testing.assert_allclose(grad_squared_hinge(x, y, beta, l), soln)\n",
    "\n",
    "for i in range(3):\n",
    "    test_grad_squared_hinge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the algorithm on a simulated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a simulated dataset\n",
    "Generate a simulated dataset with 100 observations of each of 5 classes. Each observation has 60 features, each of which has a different random mean but the same within the class. The feature means are different across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n = 100 # examples of each class\n",
    "d = 60 # features\n",
    "n_classes = 5\n",
    "classes = np.arange(n_classes) # Class labels\n",
    "\n",
    "# Generate the mean shifts for each feature of each class\n",
    "mean_feature_class = np.random.uniform(0, 1, size=(d, n_classes))\n",
    "\n",
    "# Generate values for each class\n",
    "X_class = list()\n",
    "for c in range(n_classes):\n",
    "    # Generate n examples for each of d features\n",
    "    x = [np.random.normal(mean_feature_class[j, c], 1, size=n) for j in range(d)]\n",
    "    \n",
    "    # Put them together into an (n x d) array\n",
    "    X_class.append(np.array(x).T)\n",
    "\n",
    "# Concatenate the generated features for each class\n",
    "X = np.concatenate(X_class)\n",
    "\n",
    "# Standardise the data\n",
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "\n",
    "# Generate the labels\n",
    "y = np.repeat(classes, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the linear SVM on the simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_data_ovo(x, y, pair):\n",
    "    \"\"\"Returns feature and labels of data points for only the chosen pair of classes.\n",
    "    \n",
    "    The response variables are also converted into +/- 1 labels.\n",
    "    \"\"\"\n",
    "    # Pick the data for the two classes\n",
    "    chosen = np.isin(y, pair)\n",
    "    x_chosen = x[chosen]\n",
    "    y_chosen = y[chosen]\n",
    "\n",
    "    # Convert response variables to +/- 1 labels\n",
    "    y_chosen = ((y_chosen == pair[1]) * 2 - 1).astype(x.dtype)\n",
    "    \n",
    "    return x_chosen, y_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained linear SVM achieved an accuracy of 0.98 over the training set\n"
     ]
    }
   ],
   "source": [
    "x_chosen, y_chosen = pick_data_ovo(X, y, (0, 1))\n",
    "\n",
    "betas = linear_svm(x_chosen.T, y_chosen, l=1, include_steps=True)\n",
    "\n",
    "accuracy = np.mean(np.sign(x_chosen @ betas[-1]) == y_chosen)\n",
    "print('The trained linear SVM achieved an accuracy of %.2f over the training set' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the training process, we can plot the objective function and the norm of the gradient at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Norm of gradient vs iterations')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "objective_values = [squared_hinge(x_chosen.T, y_chosen, beta=b, l=1)\n",
    "                    for b in betas]\n",
    "ax1.plot(objective_values)\n",
    "ax1.set_xlabel('Iterations')\n",
    "ax1.set_ylabel('Objective value')\n",
    "ax1.set_title('Objective values vs iterations')\n",
    "\n",
    "norm_grads = [np.linalg.norm(grad_squared_hinge(x_chosen.T, y_chosen, b, l=1))\n",
    "              for b in betas]\n",
    "ax2.plot(norm_grads)\n",
    "ax2.set_xlabel('Iterations')\n",
    "ax2.set_ylabel('Euclidean norm of the gradient')\n",
    "ax2.set_title('Norm of gradient vs iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with each iteration of gradient descent, the objective values becomes smaller and the Euclidean norm of the gradient at the current weight vector gets closer to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world dataset\n",
    "demo file, that allows a user to launch the method on a real-world dataset of your choice, visualize the training process, and print the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UCI ML hand-written digits datasets from sklearn\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b433a13208>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the data\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = random.randint(0, 100)\n",
    "\n",
    "print(\"Label: %s\" % digits.target_names[digits.target[i]])\n",
    "plt.imshow(digits.images[i], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of \"images\": (1797, 8, 8)\n",
      "Shape of \"data\": (1797, 64)\n",
      "Shape of \"target\": (1797,)\n",
      "Shape of \"target_names\": (10,)\n"
     ]
    }
   ],
   "source": [
    "# Understand the data format\n",
    "\n",
    "print('Shape of \"images\":', digits.images.shape)\n",
    "print('Shape of \"data\":', digits.data.shape)\n",
    "print('Shape of \"target\":', digits.target.shape)\n",
    "print('Shape of \"target_names\":', digits.target_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target, test_size=0.25, random_state=0, stratify=digits.target)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test = scaler.transform(X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, test our SVM code on just two of the 10 classes (0 and 1) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained linear SVM achieved an accuracy of 1.00 over the training set\n",
      "The trained linear SVM achieved an accuracy of 1.00 over the test set\n"
     ]
    }
   ],
   "source": [
    "x_train_chosen, y_train_chosen = pick_data_ovo(X_train, y_train, (0, 1))\n",
    "betas = linear_svm(x_train_chosen.T, y_train_chosen, l=1, include_steps=True)\n",
    "\n",
    "accuracy_train = np.mean(np.sign(x_train_chosen @ betas[-1]) == y_train_chosen)\n",
    "print('The trained linear SVM achieved an accuracy of %.2f over the training set' % accuracy_train)\n",
    "\n",
    "x_test_chosen, y_test_chosen = pick_data_ovo(X_test, y_test, (0, 1))\n",
    "accuracy_test = np.mean(np.sign(x_test_chosen @ betas[-1]) == y_test_chosen)\n",
    "print('The trained linear SVM achieved an accuracy of %.2f over the test set' % accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classifier using a one-vs-rest strategy\n",
    "Now that we've confirmed that the linear SVM works to differentiate between two classes, we can construct a multiclass classifier by training one linear SVM for each class. Then, to make a prediction we can simply take the class of the linear SVM corresponding to the largest output value.\n",
    "\n",
    "We will also use cross validation to select a good value for the regularisation parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_ovr_cv(x, y, svm, k, lambdas, **kwargs):\n",
    "    \"\"\"Trains a collection of SVMs as a one-vs-rest multiclass classifier.\n",
    "    Use k-fold cross validation to tune the regularisation parameter lambda.\n",
    "    :param x: features matrix (n x d)\n",
    "    :param y: labels vector (n x 1)\n",
    "    :param svm: linear SVM function\n",
    "    :param k: number of folds to use for cross-validation\n",
    "    :param lambdas: an iterable of lambdas to try with cross validation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the best lambda using cross-validation\n",
    "    scores = [_cv_mean_score(x, y, svm, k, l=l, **kwargs) for l in lambdas]\n",
    "    \n",
    "    # Favour larger values of regularisation while maximising score\n",
    "    best_lambda = lambdas[np.argmax(scores)]\n",
    "    \n",
    "    # Train the SVM using the best lambda found\n",
    "    betas, labels = train_classifier_ovr(x.T, y, svm, l=best_lambda, **kwargs)\n",
    "    \n",
    "    return betas, labels, best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def _cv_mean_score(x, y, svm, k, **kwargs):\n",
    "    \"\"\"Trains the SVM, and use k-fold cross validation to calculates its\n",
    "    mean accuracy score across all folds.\n",
    "    \"\"\"\n",
    "    \n",
    "    if (x.shape[0] != y.shape[0]):\n",
    "        raise ValueError('lengths %d and %d not aligned' % (x.shape[0], y.shape[0]))\n",
    "        \n",
    "    # Shuffle and split the training data into folds\n",
    "    x, y = shuffle(x, y)\n",
    "    x_folds = np.array_split(x, k)\n",
    "    y_folds = np.array_split(y, k)\n",
    "    \n",
    "    # Calculate the scores for each fold's cross validation\n",
    "    scores = [_cv_score_fold(x_folds, y_folds, svm, i, **kwargs)\n",
    "              for i in range(k)]\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cv_score_fold(x_folds, y_folds, svm, i, **kwargs):\n",
    "    \"\"\"Trains and scores an SVM for the i-th fold.\"\"\"\n",
    "    \n",
    "    if (len(x_folds) != len(y_folds)):\n",
    "        raise Exception('x_folds and y_folds should have the same count')\n",
    "    \n",
    "    # Extract the i-th fold of X as test set and concat the rest\n",
    "    xtrain = list(x_folds)\n",
    "    xtest = xtrain.pop(i)\n",
    "    xtrain = np.concatenate(xtrain)\n",
    "\n",
    "    # Extract the i-th fold of y as test set and concat the rest\n",
    "    ytrain = list(y_folds)\n",
    "    ytest = ytrain.pop(i)\n",
    "    ytrain = np.concatenate(ytrain)\n",
    "\n",
    "    # Standardise X\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    xtrain = scaler.fit_transform(xtrain)\n",
    "    xtest = scaler.transform(xtest)\n",
    "    \n",
    "    # Train the SVM\n",
    "    betas, labels = train_classifier_ovr(xtrain.T, ytrain, svm, **kwargs)\n",
    "    \n",
    "    # Calculate the score\n",
    "    predicted = predict_ovr(xtest.T, betas, labels)\n",
    "    \n",
    "    return np.mean(predicted == ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_ovr(x, y, svm, **kwargs):\n",
    "    \"\"\"Trains one SVM per label in a one-vs-rest strategy.\n",
    "    :param x: features matrix (d x n)\n",
    "    :param y: labels vector (n x 1)\n",
    "    :param svm: linear SVM function\n",
    "    \"\"\"\n",
    "    \n",
    "    if (x.shape[1] != y.shape[0]):\n",
    "        raise ValueError('lengths %d and %d not aligned' % (x.shape[1], y.shape[0]))\n",
    "    \n",
    "    labels = np.unique(y)\n",
    "    betas = [_train_classifier_label_ovr(x, y, svm, label, **kwargs)\n",
    "             for label in labels]\n",
    "    return betas, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_classifier_label_ovr(x, y, svm, label, **kwargs):\n",
    "    # Convert y to +/- 1 labels\n",
    "    y = ((y == label) * 2 - 1)\n",
    "    \n",
    "    # Train the SVM\n",
    "    return svm(x, y, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ovr(x, betas, labels):\n",
    "    highest_score_index = np.argmax([x.T @ beta for beta in betas], axis=0)\n",
    "    predicted = np.array([labels[idx] for idx in highest_score_index])\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the generated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 546 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lambdas = 10.**-np.arange(0, 4)\n",
    "betas, labels, best_lambda = multiclass_ovr_cv(\n",
    "    X, y, linear_svm, k=3, lambdas=lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross validation, we found the the best lambda = 1.0000\n",
      "The accuracy on the training set is 0.87\n"
     ]
    }
   ],
   "source": [
    "predicted = predict_ovr(X.T, betas, labels)\n",
    "accuracy = np.mean(predicted == y)\n",
    "    \n",
    "print('Using cross validation, we found the the best lambda = %.4f' % best_lambda)\n",
    "print('The accuracy on the training set is %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lambdas = 10.**-np.arange(0, 4)\n",
    "betas, labels, best_lambda = multiclass_ovr_cv(\n",
    "        X_train, y_train, linear_svm, k=3, lambdas=lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross validation, we found the the best lambda = 0.1000\n",
      "The accuracy on the test set is 0.93\n"
     ]
    }
   ],
   "source": [
    "predicted = predict_ovr(X_test.T, betas, labels)\n",
    "accuracy = np.mean(predicted == y_test)\n",
    "\n",
    "print('Using cross validation, we found the the best lambda = %.4f' % best_lambda)\n",
    "print('The accuracy on the test set is %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with scikit-learn\n",
    "First, try sklearn on the simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = { 'C': 10.**-np.arange(0, 4) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([1.   , 0.1  , 0.01 , 0.001])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LinearSVC(loss='squared_hinge')\n",
    "grid = GridSearchCV(classifier, parameters, cv=3)\n",
    "\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross validation, we found the the best C = 0.0010\n",
      "The accuracy on the training set is 0.87\n"
     ]
    }
   ],
   "source": [
    "print('Using cross validation, we found the the best C = %.4f' % grid.best_params_['C'])\n",
    "accuracy = grid.score(X, y)\n",
    "print('The accuracy on the training set is %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try using Scikit-Learn on the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([1.   , 0.1  , 0.01 , 0.001])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LinearSVC(loss='squared_hinge')\n",
    "grid = GridSearchCV(classifier, parameters, cv=3)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross validation, we found the the best C = 0.1000\n",
      "The accuracy on the test set is 0.97\n"
     ]
    }
   ],
   "source": [
    "print('Using cross validation, we found the the best C = %.4f' % grid.best_params_['C'])\n",
    "accuracy = grid.score(X_test, y_test)\n",
    "print('The accuracy on the test set is %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the trained Scikit-Learn model had better accuracy than my implementation.  There are several differences - our $\\lambda$ regularisation parameter is not identical to the $C$ regularisation parameter from sklearn, so the objective function being optimised is not the same. `GridSearchCV` uses a stratified split, while my implementation of cross validation uses a fully randomised shuffle and split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
